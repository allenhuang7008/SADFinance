{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Multiprocessing\n",
    "\n",
    "Multiprocessing is a useful python package that enables users to utilize multiple processors on a given machine for more efficient progress. The Pool object allows the exploitation of data parallelism by distributing the work across a pool of processes running the same function. This greatly improves the speed at which the work is done, reducing overall runtime. \n",
    "\n",
    "Multiprocesing is mainly preferred when calling functions on larger sets of data expressing data parallelism. Data parallelism is the concept of breaking a set of data into smaller sets, which is then processed on multiple processes applying the same function without communicating with each other. Joining the output of these processes should produce the same result as if one process had applied the function to the entire dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for parsing data\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from html.parser import HTMLParser\n",
    "import os\n",
    "import spacy\n",
    "from lxml import etree\n",
    "import itertools\n",
    "from itertools import repeat\n",
    "\n",
    "# Libraries for importing our sentiment analysis models\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sentence_transformers import SentenceTransformer as ST\n",
    "\n",
    "# Libraries for Multiprocessing\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "from time import perf_counter\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set corpus to the folder of files you want to use\n",
    "corpus = '/home/ec2-user/SageMaker/data/WSJ_full/'\n",
    "\n",
    "# Read in files\n",
    "input_files = os.listdir(corpus)\n",
    "\n",
    "print(\"Loaded\", len(input_files), \"documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Total Cores\n",
    "Check the total number of cores on your current device. The following multiprocessing portions will be using this variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core count\n",
    "num_cores = mp.cpu_count()\n",
    "print(num_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve metadata from XML document\n",
    "def getxmlcontent(corpus, file, strip_html=True):\n",
    "    try:\n",
    "        tree = etree.parse(corpus + file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        if root.find('.//GOID') is not None:\n",
    "            goid = root.find('.//GOID').text\n",
    "        else:\n",
    "            goid = None\n",
    "        \n",
    "        if root.find('.//Title') is not None:\n",
    "            title = root.find('.//Title').text\n",
    "        else:\n",
    "            title = None\n",
    "        \n",
    "        if root.find('.//NumericDate')') is not None:\n",
    "            date = root.find('.//NumericDate').text\n",
    "        else:\n",
    "            date = None\n",
    "\n",
    "        if root.find('.//PublisherName') is not None:\n",
    "            publisher = root.find('.//PublisherName').text\n",
    "        else:\n",
    "            publisher = None\n",
    "        \n",
    "        if root.find('.//MpubId') is not None:\n",
    "            pubid = root.find('.//MpubId').text\n",
    "        else:\n",
    "            pubid = None\n",
    "\n",
    "        if root.find('.//FullText') is not None:\n",
    "            fulltext = root.find('.//FullText').text\n",
    "        else:\n",
    "            fulltext = None\n",
    "        \n",
    "        elif root.find('.//HiddenText') is not None:\n",
    "            text = root.find('.//HiddenText').text\n",
    "        \n",
    "        elif root.find('.//Text') is not None:\n",
    "            text = root.find('.//Text').text\n",
    "        \n",
    "        elif root.find('.//AbsText') is not None:\n",
    "            text = root.find('.//AbsText').text\n",
    "        \n",
    "        else:\n",
    "            text = None\n",
    "        \n",
    "        # Strip html from text portion\n",
    "        if text is not None and strip_html == True:\n",
    "            text = strip_tags(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while parsing file {file}: {e}\")\n",
    "    \n",
    "    return goid, title, date, publisher, pubid, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for cleaning up text\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.text = StringIO()\n",
    "\n",
    "    def handle_data(self, d):\n",
    "        self.text.write(d)  \n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.text.getvalue()\n",
    "    \n",
    "def strip_tags(html):\n",
    "    \"Remove HTML tags from the provided html text\"\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the necesssary content from the XML files\n",
    "# Set up for multiprocessing -- for a split list of files\n",
    "def parse_xmls(document_list):\n",
    "\n",
    "    sents = []\n",
    "    goids = []\n",
    "    dates = []\n",
    "    pubids = []\n",
    "\n",
    "    nlp = spacy.load('../../Resources/Models/en_core_web_sm-3.2.0/en_core_web_sm/en_core_web_sm-3.2.0')\n",
    "\n",
    "    try:\n",
    "        for document in document_list:\n",
    "\n",
    "            sents_short = []\n",
    "            goids_short = []\n",
    "            dates_short = []\n",
    "            pubids_short = []\n",
    "\n",
    "            #Get necessary metadata\n",
    "            goid, title, date, publisher, pub_id, text = getxmlcontent(corpus, document, strip_html=True)\n",
    "\n",
    "            if text is not None:\n",
    "                all_sents = nlp(text)\n",
    "\n",
    "                # Make list of all sentences\n",
    "                sentences = []\n",
    "\n",
    "                for sent in all_sents.sents:\n",
    "                    sent_text = sent.text\n",
    "\n",
    "                    if sent_text is None:\n",
    "                        continue\n",
    "                    sent_text = sent_text.strip()\n",
    "                    if not sent_text:\n",
    "                        continue\n",
    "\n",
    "                    sentences.append(sent_text)\n",
    "\n",
    "                sents_short.extend(sentences)\n",
    "                goids_short.extend([goid] * len(sentences))\n",
    "                dates_short.extend([date] * len(sentences))\n",
    "                pubids_short.extend([pub_id] * len(sentences))\n",
    "\n",
    "                # Combine all individual document info into one big list\n",
    "                sents.extend(sents_short)\n",
    "                goids.extend(goids_short)\n",
    "                dates.extend(dates_short)\n",
    "                pubids.extend(pubids_short)\n",
    "\n",
    "    except AttributeError:\n",
    "        # Error logging - will show filename if there is a problem processing it\n",
    "        print(\"Attribute Error\" + document)\n",
    "\n",
    "    return sents, goids, dates, pubids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(input_files):\n",
    "    # When using multiple processes, important to eventually close them to avoid memory/resource leaks\n",
    "    try: \n",
    "        # Record duration of operation\n",
    "        # start = perf_counter()\n",
    "\n",
    "        # Define a thread Pool to process multiple XML files simultaneously\n",
    "        # Default set to num_cores, but may change number of processes (cores_used) depending on instance\n",
    "        cores_used = num_cores - 1\n",
    "        p_parse = Pool(processes = cores_used)\n",
    "\n",
    "        # Apply function with Pool to corpus, array is split into smaller lists for faster progress\n",
    "        split = np.array_split(input_files, num_cores)\n",
    "        processed_lists = p_parse.map(parse_xmls, split)\n",
    "\n",
    "        # end = perf_counter()\n",
    "\n",
    "        #total_minutes = (end - start) / 60\n",
    "        #total_seconds = (end - start) % 60\n",
    "\n",
    "        #print(f\"Took {int(total_minutes)}min {total_seconds :.2f}s to parse {len(input_files)} documents. \")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while parsing documents: {e}\")\n",
    "\n",
    "    finally:\n",
    "        p_parse.close()\n",
    "\n",
    "    # Convert to dataframe to extract individual lists\n",
    "    df = pd.DataFrame(processed_lists, columns=['sentences', 'goids', 'dates', 'pubids'])\n",
    "\n",
    "    sent = df['sentences'].to_list()\n",
    "    parsed_sents = list(itertools.chain.from_iterable(sent))\n",
    "\n",
    "    goids = df['goids'].to_list()\n",
    "    parsed_goids = list(itertools.chain.from_iterable(goids))\n",
    "\n",
    "    dates = df['dates'].to_list()\n",
    "    parsed_dates = list(itertools.chain.from_iterable(dates))\n",
    "\n",
    "    pubids = df['pubids'].to_list()\n",
    "    parsed_pubids = list(itertools.chain.from_iterable(pubids))\n",
    "\n",
    "    # Make sure all arrays are the same length\n",
    "    assert len(parsed_sents) == len(parsed_goids) == len(parsed_dates) == len(parsed_pubids), 'parsed data have unequal lengths'\n",
    "\n",
    "    return parsed_sents, parsed_goids, parsed_dates, parsed_pubids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Emotion Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the encodings that we will use\n",
    "label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "label_encodings = [\"love\", \"anger\", \"disgust\", \"fear\", \"happiness\", \"sadness\", \"surprise\", \"neutral\", \"other\"]\n",
    "label_encodings.sort()\n",
    "label_encoder.fit(label_encodings)\n",
    "\n",
    "# Print out the emotions to check it has been loaded successfully\n",
    "emotions = label_encoder.classes_\n",
    "print(emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Classifier and Scaler\n",
    "Load logistic regression classifier and pre-fit scaler and store the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set classifier_path to location of the logistic regression classifier\n",
    "classifier_path = '../../Resources/Models/nli-mpnet-base-v2-LR-classifier.pkl'\n",
    "\n",
    "try:\n",
    "    # Load and store model in sentiment_model\n",
    "    file = open(classifier_path, 'rb')\n",
    "    sentiment_model = pickle.load(file)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error while opening file: {e}\")\n",
    "\n",
    "finally:\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set scaler_path to location of the logistic regression classifier\n",
    "scaler_path = '../../Resources/Models/sentimentScaler.pkl'\n",
    "\n",
    "try:\n",
    "    # Load and store model in sentiment_model\n",
    "    file = open(scaler_path, 'rb')\n",
    "    scaler = pickle.load(file)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error while opening file: {e}\")\n",
    "\n",
    "finally:\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Labels with Multiprocessing\n",
    "By using SBERT model and the array of parsed sentences, we will now output a corresponding array where each element is a tuple of the predicted label and an array of the raw possibilities for each label. The emotions ouputted should match that of the previous cell.\n",
    "\n",
    "This process uses multiprocessing to efficiently run large-sized corpuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set sbert_path to location of SBERT model\n",
    "sbert_path = '../../Resources/Models/nli-mpnet-base-v2'\n",
    "transformer = ST(sbert_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding through multiprocessing\n",
    "def encode_sentence(sent):\n",
    "\n",
    "    # Encode chunk of sentences in parsed_sents array\n",
    "    sentence_embedding = transformer.encode(sent, show_progress_bar=False)\n",
    "\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_process(parsed_sents):\n",
    "    # When using multiple processes, important to eventually close them to avoid memory/resource Leaks\n",
    "\n",
    "    try:\n",
    "        # start = perf_counter()\n",
    "\n",
    "        # Define a thread Pool to process multiple sentences simultaneously\n",
    "        # Default set to num_cores, but may change nubmer of processes depending on instance \n",
    "        cores_used = num_cores - 1\n",
    "        p_encode = Pool(processes=cores_used)\n",
    "\n",
    "        # Apply function with Pool to array\n",
    "        chunksize = int(len(parsed_sents)/cores_used)\n",
    "        sentence_embeddings = p_encode.map(encode_sentence, parsed_sents, chunksize)\n",
    "\n",
    "        # end = perf_counter()\n",
    "\n",
    "        # total_minutes = (end - start) / 60\n",
    "        # total_seconds = (end - start) % 60\n",
    "\n",
    "        #print(f\"Took {int(total_minutes)}min {total_seconds:.2f}s to encode {len(parsed_sents)} sentences\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while encoding sentences: {e}\")\n",
    "\n",
    "    finally:\n",
    "        p_encode.close()\n",
    "\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(sentence_embeddings):\n",
    "    # Both scaler and sentiment_model should exist before running this cell\n",
    "    if scaler is not None and sentiment_model is not None:\n",
    "        standardized = scaler.transform(sentence_embeddings)\n",
    "\n",
    "        #y_pred_numeric = sentiment_model.predict(standardized)\n",
    "        #y_pred_string = label_encoder.inverse_transform(y_pred_numeric)\n",
    "\n",
    "        # Call the predict function on our sentences\n",
    "        raw_predictions = sentiment_model.predict_proba(standardized)\n",
    "\n",
    "        #results = list(zip(y_pred_string, raw_predictions))\n",
    "\n",
    "    else:\n",
    "        print(\"Please load scaler and sentiment model.\")\n",
    "    \n",
    "    return raw_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataframe\n",
    "Create a dataframe to show the predicted labels and each estimated value for each emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_mean(raw_predictions, parsed_goids, parsed_dates, parsed_pubids, parsed_sents):\n",
    "    # Make column list\n",
    "    columns = ['GOID', 'Date', 'PubID', 'Sentence']\n",
    "    columns.extend(emotions)\n",
    "\n",
    "    # Create dictionary for dataframe\n",
    "    data = {}\n",
    "\n",
    "    # map meta data\n",
    "    data['GOID'] = parsed_goids\n",
    "    data['Date'] = parsed_dates\n",
    "    data['Pub ID'] = parsed_pubids\n",
    "    data['Sentence'] = parsed_sents\n",
    "    for i, emotion in enumerate(emotions):\n",
    "        data[emotion] = raw_predictions[:, i]\n",
    "    results_df = pd.DataFrame(data=data, columns=columns)\n",
    "\n",
    "    # Create document-level dataframe\n",
    "    means_df = results_df.groupby(['GOID'], as_index=True).agg({'Date': 'first', \n",
    "                                                                'anger': 'mean',\n",
    "                                                                'disgust': 'mean',\n",
    "                                                                'fear': 'mean',\n",
    "                                                                'happiness': 'mean',\n",
    "                                                                'love': 'mean',\n",
    "                                                                'neutral': 'mean',\n",
    "                                                                'other': 'mean',\n",
    "                                                                'sadness': 'mean',\n",
    "                                                                'surprise': 'mean'})\n",
    "    \n",
    "    return means_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'total number of docs to process: {len(input_files)}\\n')\n",
    "\n",
    "scale=1000\n",
    "interval = np.arange(0, len(input_files), scale)\n",
    "for i in tqdm(range(len(interval))):\n",
    "    parsed_sents, parsed_goids, parsed_dates, parsed_pubids = parser(input_files[interval[i] : interval[i]+scale])\n",
    "    sentence_embeddings = encoding_process(parsed_sents)\n",
    "    raw_predictions = prediction(sentence_embeddings)\n",
    "\n",
    "    means_df = doc_mean(raw_predictions, parsed_goids, parsed_dates, parsed_pubids, parsed_sents)\n",
    "\n",
    "    # Save output to file\n",
    "    means_df.to_csv(f'SA_results/doc_score_full.csv', mode='a', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
